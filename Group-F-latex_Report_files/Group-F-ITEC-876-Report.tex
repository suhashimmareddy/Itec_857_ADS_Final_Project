\documentclass[a4paper,1pt]{article}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{titling}
\usepackage[margin=1.3in]{geometry}
\usepackage{sectsty}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\begin{document}
	
	
	\begin{titlepage}
		
		\centering
		\includegraphics{mac-download.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
	
		{\scshape\LARGE ITEC-876-Applications of Data Science \par}
		\vspace{1cm}
		
		{\Large \textbf{Default Project-GROUP-F}\par}
		\vspace{1cm}
		{\Large 45667446-K.Sai prasad  \space\space\space\space\space\space\space       45693242-Suhash Reddy I \par}
		\vspace{1cm}
			{\Large 45692866-Praveen Kumar Ulasa \space\space\space\space\space\space\space       45516065-Vaibhav Agarwal \par}
		\date{}
		
		\vfill
		\date{}
		% Bottom of the page
		{\large \today\par}
	\end{titlepage}
	
	
	
	\date{}
	
	
		
\setlength{\droptitle}{-14em}\bigskip





\begin{center}
	\textbf{\underline {Introduction}}
\end{center}

 \noindent The main goal of the project is to apply text classification on the problems which we face in day to day activities like spam detection, credit defaulters etc. For the project, we have used a library called as “FastText” which is an efficient way of learning word representations and sentence clarifications. We have used different types of language models namely: Epoch, learning Rate, N-grams, Hierarchical SoftMax and Multilabel Classification to improve the precision (efficiency) of the outcome.\bigskip
	\\•	\textbf{Epoch}:  By default, FastText trains our data only 5 times so to improve the quality of the outcome we have changed the parameters of Epoch to get higher accuracy.\\
	\\•	\textbf{Learning Rate}: To improve the learning rate of the algorithm we tune with the parameters of the Learning Rate, ideally it is in between 0.1 To 1.0.\\
	\\•	\textbf{N-grams}: By using the word n-gram, we can improve the precision of preprocessing of the data. N-grams works on the concept that unigrams will check by taking a single word at a time, bigrams will scan 2 words at a time and similarly the process continues.\\
	\\•	\textbf{Hierarchical SoftMax}: It works as the tree hierarchy, where each word is represented by leaves or parent nodes; and internal nodes represents the probabilities of child node, we use this function for faster computation of larger datasets \\
	\\•	\textbf{Multilabel Classification}: When number of labels are used to predict the threshold of predicted probability; here we tune arguments to improve the efficiency of the data.

\begin{center}
	\textbf{\underline {Replication of Original Work}}
\end{center}

 
\noindent We tried to replicate the output on the cooking dataset, which is given in fast text classification tutorial, here are the snippets of the replicated output on the original data\\

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 9"}
	\caption{}
	\label{fig:precision-and-recall-at-position-9}
\end{figure}

\begin{figure}[h]
	\centering

\label{fig:precision-and-recall-at-position-10}
\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 10"}
\caption{Precision Before preprocessing the text file}
\end{figure}

\begin{figure}[h]
	\centering

	\label{fig:Precision and Recall at position 11}
	\includegraphics[width=1.0\linewidth]{"screenshot002"}
		\caption{	Precision After preprocessing}
\end{figure}



\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 13"}
		\captionof{figure}{Precision at Epoch 25}
	\end{minipage}
\end{center}


\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 14"}
		\captionof{figure}{	Precision at learning rate 1.0}
	\end{minipage}
\end{center}


\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 15"}
		\captionof{figure}{Precision at learning rate 1.0 \& epoch 25}
	\end{minipage}
\end{center}


\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 16"}
		\captionof{figure}{Precision at learning rate 1.0, epoch 25 \& wordngram2}
	\end{minipage}
\end{center}


\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 17"}
		\captionof{figure}{	Multilabel }
	\end{minipage}
\end{center}


\noindent Upon Looking at the results from the Original dataset, the precision of the pre-processed data (which is model efficiency in identifying the labels) is increased when compared with the precision of the data set before pre-processing.  we can see that Precision@1 is 0.139, and after we pre-processed the data i.e, converting all the upper to lower, removing special characters, we find the Precision to increase by 3.5\%, which is now at 0.174.\\

\noindent When we ask the question:” Why not put the knives in the dishwasher”, the 5 predicted labels by the model were given as follows: “baking, food-safety, equipment, bread, substitution”; only one label out of those predicted labels is correct so Precision is 0.2. Out of the 3 correct labels only one is predicted, so leaving Recall at 0.03.

\paragraph{Epoch:}{After comparing the precision with the pre-processed data, we can see that the Precision has improved by 34.3\%, which is very drastic change.}

\paragraph{Learning Rate:}{we changed the learning rate and compared the result with the Epoch, where we have seen further improvement in precision by 5.9\%.}

\paragraph{Combining Epoch \& Learning Rate:}{With the change in Epoch and learning rate, we saw improvement in the Precision, hence we tried to bring both together and yet we saw Precision to improve a bit more.}

\paragraph{Word gram:}{To improve the efficiency furthermore we have used bigram that has  improvement in the Precision by 2.5\%.}\\

\noindent Now after running the multi label model on the original data, we had given the same question asked as above and it predicted better labels as compared to before.\\

\noindent$\Rightarrow$We learned that with increase in Epoch, learning rate, and word gram the precision will also increase.\\

\noindent$\Rightarrow$ Multilabel model will try to improve in predicting more no of original labels when compared with the initial model Prediction.\\ 

\noindent$\Rightarrow$	We use the trends that are observed from the original data set as the base line comparison for all our data sets.\\

\noindent$\Rightarrow$	We will be trying to explore these models in the coming sections with some parameter tuning and compare the Precision at each stage, to find which model is doing better.\\


\noindent\textbf{Operating system:}  Linux, Windows\\
\textbf{Ram:} 8GB\\
\textbf{Processors:} 4 cores\\
\textbf{Software}: Jupyter from Anaconda to be installed on the windows or Linux and Fast Text to be Installed in the Linux (Make sure that before fast text install you do have an updated version of c compiler)\\

\noindent\textbf{Packages to be Downloaded in Jupyter:} Beautiful Soup 4.
Distributed Version Control: Git hub (All the work will be uploaded to the git hub Repository)\\
\url{https://github.com/suhashimmareddy/Itec_857_ADS_Final_Project}\\
The above is the git hub link where you can find the files. 

	
	\begin{center}
		\textbf{\underline{Construction of new data:}}
	\end{center}
	
\noindent We have used python programming language with beautiful soup to establish the connection with stack exchange and extracted the data by web scrapping using the html parser. We are in need of question  and tags from the division section that are used for the text classification as label and question in the later part of the analysis.\\

\noindent The question and tags were extracted and directly converted them to a text file that fast text can understand, which is in the form of labels and questions.\\

\noindent Below is the snippet of the data which is before scrapping from the stack exchange. \\

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 18"}
	
	\end{minipage}
\end{center}



\noindent The snippets of all the 4 datasets which has been scrapped from the stack exchange and converted into the fast text format is given below:\\

\paragraph{Dataset 1 : Music \& movies}

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 19"}
		
	\end{minipage}
\end{center}



\paragraph{Dataset 2 : Python}

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 20"}
		
	\end{minipage}
\end{center}

\paragraph{Dataset 3 : Android}

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 21"}
		
	\end{minipage}
\end{center}

\paragraph{Dataset 4 :  Electronics}

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 22"}
		
	\end{minipage}
\end{center}

\begin{center}
	\textbf{\underline{Different types of labels in each dataset}}
\end{center}

\paragraph{Dataset 1-Music \& movies:} The data have been scrapped from stack exchange where we have all the questions which are related to the music and movies and we have merged the music and movies data and got approximately 35000 of questions which are used for the fast Text analysis .\\
In the below histogram we can get the count of labels per question. The number of questions which are having 2 labels is around 14000 being highest. 

\paragraph{Dataset 2-Python}We scrapped these data from stack exchange where we have all the questions related to the python and got approximately of 51000 of questions which are used for the fast Text analysis.In the below histogram we can get the count of labels per question. The number of questions which are having 3 labels are around 35000 being highest. 




\paragraph{Dataset 3-Android}The Android data is scrapped from stack exchange that has questions related to the android OS and got approximately of 50000 of questions which are used for the fast Text analysis.The histogram plotted below represents the count of labels per question. The number of questions which are having 2 labels are around 15000 being highest.



\paragraph{Dataset 4-Electronics}We got this electronic dataset from the stack exchange as well with questions related to electronics and got approximately 50000 questions which are used for the fast Text analysis.In the below histogram we can get the count of labels per question. The number of questions which are having 2 labels are around 16000 being highest. 


\begin{figure}[h!]
	\captionsetup[subfigure]{labelformat=empty}
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=0.8\linewidth]{"Precision and Recall at position 23"}
		\caption{Dataset 1-Music \& movies}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
			\includegraphics[width=0.8\linewidth]{"Precision and Recall at position 24"}
		\caption{Dataset 2-Python}
	\end{subfigure}
\end{figure}

\begin{figure}[h!]
	\captionsetup[subfigure]{labelformat=empty}
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=0.8\linewidth]{"Precision and Recall at position 25"}
		\caption{Dataset 3-Android} 
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=0.8\linewidth]{"Precision and Recall at position 26"}
		\caption{Dataset 4-Electronics}
	\end{subfigure}
\end{figure}

\pagebreak

\begin{center}
	\textbf{\underline {Results on new data}}
\end{center}

\noindent{In this section we are showing the results from three Data sets mainly Music-Movies, Python and Electronics. All the Data sets results can be found in the supporting section in their respective folders. On Dataset Python when we run fast text with just the default parameters the precision@1 is coming as 0.997, it’s because there is a common label named label\_python for all the questions so the model is predicting the correct label at almost 100\% all the time, so we decided to look at precision@2, that has given the precision at 0.644, and precision@3 is 0.464.}\\


\noindent{we performed tests with all the models on this Python Data Set and compared the precision and Recall value at position 2 for the comparisons to be valid. First, we cleaned the dataset by converting all the upper cases letters to lower case letters (Since Both and upper case would represent the same meaning) as well as removing some special characters.}\\

\noindent{After Doing some pre-processing the precision at 2 is coming as 0.694 (precision is increased by 5\% when compared this with precision at 2 before pre-processed) and precision at 3 is 0.505. There are several ways we can increase the efficiency of fast text some of them are Epoch, Learning rate, Word-N-Gram, Multilabel, Hierarchical SoftMax.}

\begin{center}
	\textbf{\underline{Epoch}}
\end{center}

\noindent{Below are the graph’s that represent Precision and Recall at 2, 3 for different number of Epoch’s (Results are extracted from Python Dataset)}\\

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"Precision and Recall at position 27"}
		
	\end{minipage}
\end{center}

\noindent{From the graph above, we can clearly see that the Precision@2 and recall@2 rate for python dataset is increasing with increase in number of Epochs (Which is no of times the program will look at the data sets) until Epoch 50, later on we can see decrease in precision by a bit.  }\\

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"Precision and Recall at position 28"}
		
	\end{minipage}
\end{center}

\noindent{The precision@3 and Recal@3 is increasing with increase in the number of Epochs as what we have seen from the original data set. }\\
\noindent We can expect this kind of increasing trend in precision, since we have seen this from running the original data set because with the increase in number of time a model can look at the data sets would make the model better in predicting the labels, so we can expect this.


\begin{center}
	\textbf{\underline{Learning Rate}}
\end{center}

\noindent{Below are the graphs that represent Precision and Recall at for different number of Learning rates (Results are extracted from Music and Movies Dataset)}\\

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=0.5\linewidth]{"Precision and Recall at position 29"}
		
	\end{minipage}
\end{center}

\noindent{From The graph above we can see that with increase in Learning rate Number there is an increase in Precision and Recall. From the above two observations we can see that Precision is Directly Proportional to the Learning rate and Epoch.}\\ 

\noindent{ Now we try to look at the results when we have Both Learning rate and Epoch.}\\

\pagebreak

\begin{center}
	\textbf{\underline{Learning rate \& epoch:}}
\end{center}

\noindent{ Below are the graphs that represent Precision and Recall at 2,3 for different number of Learning rates and Epoch (Results are extracted from Python Dataset)}\\


\begin{figure}[h!]
	
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=1.0\linewidth]{"screenshot003"}
		
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=1.0\linewidth]{"screenshot004"}
		
	\end{subfigure}
\end{figure}


\begin{figure}[h!]
	
	\centering
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=1.0\linewidth]{"screenshot005"}
	 
	\end{subfigure}
	\begin{subfigure}[b]{0.4\linewidth}
		\includegraphics[width=1.0\linewidth]{"screenshot006"}
		
	\end{subfigure}
\end{figure}


The above are the graphs representing the Precision and Recall at position 2,3 for different learning rates when epoch 10, 25. 
Now when we look at Precision@2, Recall@2 with different learning and epoch, we generally expect an increase in precision and recall but here it’s quite opposite that the precision and recall are kind of reducing/Decreasing by very small amount with increase in learning and epoch. We think that it’s because this python data set has a same label named python which is common for all the questions along with that there may be some other label which is even common to all but not as common as the label\_Python. So, we decided to look at precision 3
And when we do that (Graphs with precision@3 and Recall@3 for epoch 10, 25), we can see that there is an increase in Both Precision and Recall with increase in Learning Rate and Epoch.

\begin{center}
	\textbf{\underline{Word n grams}}
\end{center}

\noindent The followig results are from the Electronics Data sets.\\

\noindent Usually with increase in Word N Grams the precision should increase but When running word n grams 2 with epoch 25 and learning rate 1.0 we can see the precession rate is 0.50 and recall 0.18 and with increase in the wordNgram to 3 we can observe the precession is reduced to 0.48 and recall to 0.17 and it is  because of  the label problem in the dataset Electronics.



\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 6"}
		
	\end{minipage}
\end{center}

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 7"}
		
	\end{minipage}
\end{center}

\begin{center}
	\textbf{\underline{Multilabel}}
\end{center}

\noindent The Following outputs are from the Python Data Sets

\begin{center}
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[width=1.0\linewidth]{"Precision and Recall at position 8"}
		
	\end{minipage}
\end{center}

\noindent Since we got multilabel data we tried to use multilabel functions loss one-vs-all to get a better Precision. From the above, we can clearly see that Precision rate increased from 0.847 to 0.877 with increase in word n gram from 2 to 3. We can even see that the model is now good at predicting all the labels with the respective probability assigned to each of the labels.\\



\noindent\textbf{\underline{Conclusion:}}
After looking at the outputs from all the models, we can to a conclusion that multilabel model performs in detecting the labels.\\
So, including Multilabel Model with the best Epoch, learning rate, Word-ngram, would help the model furthermore in predicting all the labels more accurately.\\
If there are no or not many Multilabel’s in the data set initial model will also perform better.


\end{document}